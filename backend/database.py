# File: database.py

import os
from typing import Optional, Generator, Dict # Needed for the session generator and JSON field
from sqlmodel import Field, SQLModel, create_engine, Session, JSON, Column # Key SQLModel imports
from datetime import datetime # For timestamps

# --- Database File Location and URL Definition ---

# Filename for the SQLite database
DATABASE_FILE = "pcap_anonymizer.db"
# Get the directory path where this script (database.py) is located
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
# Create the connection URL for SQLite
# sqlite:/// means a relative path from the current working directory when running
# We use an absolute path here to avoid issues with the execution directory
DATABASE_URL = f"sqlite:///{os.path.join(BASE_DIR, DATABASE_FILE)}"

# --- Database Engine ---

# The 'engine' is the primary interface to the database.
# connect_args={"check_same_thread": False} is required ONLY for SQLite
# to allow its use with FastAPI (which uses different threads per request).
engine = create_engine(DATABASE_URL, echo=False, connect_args={"check_same_thread": False})
# echo=False will prevent SQLModel from printing every SQL statement


# --- Table Model Definition using SQLModel ---

# This class defines BOTH the database table structure
# and the Pydantic model for API validation and serialization.
class PcapSession(SQLModel, table=True):
    # The ID will be our generated UUID as a string. It's the primary key.
    id: str = Field(default=None, primary_key=True) # We'll use the UUID generated by us

    # User-provided name for this session/upload
    name: str = Field(index=True, nullable=False) # index=True speeds up lookups by name

    # Optional user-provided description
    description: Optional[str] = Field(default=None)

    # Original filename of the uploaded PCAP
    original_filename: Optional[str] = Field(default=None)

    # Timestamp of when the session was uploaded/created
    # default_factory ensures a new timestamp is generated whenever a record is created
    upload_timestamp: datetime = Field(default_factory=datetime.utcnow, nullable=False)

    # Path where the original .pcap file is stored (relative or absolute)
    pcap_path: str = Field(nullable=False)

    # Path where the rules .json file is stored (could be removed if rules are stored in DB)
    rules_path: Optional[str] = Field(default=None)

    # Timestamp of the last update (useful for sorting/tracking changes)
    # 'sa_column_kwargs' passes arguments directly to the underlying SQLAlchemy column
    # The onupdate here is database-level (behavior might vary across DB backends, works in SQLite)
    # updated_at: Optional[datetime] = Field(
    #     default=None, sa_column_kwargs={"onupdate": datetime.utcnow}
    # )
    # Simpler, more compatible alternative if DB-level onupdate isn't strictly needed:
    updated_at: Optional[datetime] = Field(default_factory=datetime.utcnow)

    # --- Fields for Transformed PCAPs (Task 3) ---
    is_transformed: bool = Field(default=False, index=True)
    # Link back to the original session if this is a transformed one
    original_session_id: Optional[str] = Field(default=None, foreign_key="pcapsession.id", index=True)
    # Link to the async job that created this transformed session
    async_job_id: Optional[int] = Field(default=None, foreign_key="asyncjob.id", index=True)


# --- Async Job Table Model (Task 2) ---

class AsyncJob(SQLModel, table=True):
    id: Optional[int] = Field(default=None, primary_key=True) # Auto-incrementing integer ID
    session_id: str = Field(foreign_key="pcapsession.id", index=True, nullable=False) # Original session
    job_type: str = Field(index=True, nullable=False) # e.g., 'transform', 'dicom_extract'
    status: str = Field(default="pending", index=True, nullable=False) # 'pending', 'running', 'cancelling', 'completed', 'failed', 'cancelled'
    stop_requested: bool = Field(default=False, nullable=False) # Flag to signal task cancellation
    progress: int = Field(default=0)
    # Store DICOM results as JSON. Use Column(JSON) for cross-DB compatibility.
    result_data: Optional[Dict] = Field(default=None, sa_column=Column(JSON))
    error_message: Optional[str] = Field(default=None)
    # Add trace_name to store the user-friendly name associated with the session at job creation time
    trace_name: Optional[str] = Field(default=None, description="User-provided name for the session/trace at the time of job creation")
    created_at: datetime = Field(default_factory=datetime.utcnow, nullable=False)
    updated_at: Optional[datetime] = Field(default_factory=datetime.utcnow) # Consider adding onupdate logic if needed


# --- Function to Create the Database and Tables ---

def create_db_and_tables():
    """Creates the database file and all tables defined by SQLModel models."""
    print("Attempting to create database and tables...")
    # SQLModel.metadata contains info about all classes inheriting from SQLModel with table=True
    # create_all creates them in the database connected via the engine if they don't already exist.
    SQLModel.metadata.create_all(engine)
    print("Database and tables should be created if they didn't exist.")


# --- Database Session Management (FastAPI Pattern) ---

def get_session() -> Generator[Session, None, None]:
    """
    FastAPI dependency generator to get a database session per request.
    Ensures the session is always closed, even if errors occur.
    """
    # Session(engine) creates a session object bound to our engine
    with Session(engine) as session:
        try:
            yield session # Provide the session to the endpoint function
        finally:
            # This finally block ensures the session is closed at the end of the request
            session.close()
